---
title: "K-Means Clustering & Hierarchical Clustering"
fig_width: 3
output:
  pdf_document:
    toc: yes
  html_document:
    highlight: tango
    number_sections: no
    theme: cosmo
    toc: yes
subtitle: STAT 446/646 Statistical Bioinformatics
fig_height: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### K-Means Clustering

In this exercise we use K-Means clustering on randomly generated data using the [`kmeans()`](http://bit.ly/R_kmeans) function.

```{r}
set.seed(2)
x <- matrix(rnorm(50 * 2), ncol = 2)
x[1:25, 1] <- x[1:25, 1] + 3
x[1:25, 2] <- x[1:25, 2] - 4
```

Let's start by clustering the data into two clusters with `K = 2`.
k: # of clusters
nstart = starting point for certain results (like set.seed)

```{r}
km.out <- kmeans(x, 2, nstart = 20)
```

The [`kmeans()`](http://bit.ly/R_kmeans) function returns the cluster assignments in the `cluster` component.

```{r}
km.out$cluster
```

Now let's plot the clusters.

```{r}
plot(x, col = (km.out$cluster + 1), main = "K-Means Clustering Results with K=2", xlab = "", ylab = "", pch = 20, cex = 2)
```

We can run K-means with different values for the number of clusters such as `K = 3` and plot the results.

```{r}
set.seed(4)
km.out <- kmeans(x, 3, nstart = 20)
km.out
```

```{r}
plot(x, col = (km.out$cluster + 1), main = "K-Means Clustering Results with K=3", xlab = "", ylab = "", pch = 20, cex = 2)
```

We can control the initial cluster assignments with the `nstart` argument to [`kmeans()`](http://bit.ly/R_kmeans).
  
Last line: total within

```{r}
set.seed(3)
km.out <- kmeans(x, 3, nstart = 1)
km.out$tot.withinss
km.out <- kmeans(x, 3, nstart = 20)
km.out$tot.withinss
```

### Hierarchical Clustering

We can use hierarchical clustering on the dataset we generated in the previous exercise using the [`hclust()`](http://bit.ly/R_hclust) function.  
should specify the method (default varies from function to function)

```{r}
hc.complete <- hclust(dist(x), method = "complete")
```

The [`hclust()`](http://bit.ly/R_hclust) function supports various agglomeration methods including "single", "complete", and "average" linkages.

```{r}
hc.average <- hclust(dist(x), method = "average")
hc.single <- hclust(dist(x), method = "single")
```

We can compare the different linkages by plotting the results obtained with different methods.

```{r}
par(mfrow = c(1, 3))
plot(hc.complete, main = "Complete Linkage", xlab = "", sub = "", cex = 0.9)
plot(hc.average, main = "Average Linkage", xlab = "", sub = "", cex = 0.9)
plot(hc.single, main = "Single Linkage", xlab = "", sub = "", cex = 0.9)
```

We can cut the tree into different groups using the [`cutree()`](http://bit.ly/R_cutree) function.

```{r}
cutree(hc.complete, 2)
cutree(hc.average, 2)
cutree(hc.single, 2)
```

```{r}
cutree(hc.single, 4)
```

We can scale the dataset before it to the clustering algorithm by first calling [`scale()`](http://bit.ly/R_scale).

```{r}
xsc <- scale(x)
plot(hclust(dist(xsc), method = "complete"), main = "Hierarchical Clustering with Scaled Features ")
```

should check if 1-cor(t(x)) is n x n

```{r}
x <- matrix(rnorm(30 * 3), ncol = 3)
dd <- as.dist(1 - cor(t(x)))
plot(hclust(dd, method = "complete"), main = "Complete Linkage with Correlation -Based Distance", xlab = "", sub = "")
```

### Clustering the Observations of the NCI60 Data

In this final exercise we use Hierarchical and K-means clustering on the NC160 dataset. We first scale the data to have a zero mean and standard deviation of one.

```{r}
library(ISLR)
nci.labs <- NCI60$labs
nci.data <- NCI60$data
sd.data <- scale(nci.data)
```

We run Hierarchical clustering with different linakges and plot the results.

```{r}
par(mfrow = c(1, 3))
data.dist <- dist(sd.data)
plot(hclust(data.dist), labels = nci.labs, main = "Complete Linkage", xlab = "", sub = "", ylab = "")
plot(hclust(data.dist, method = "average"), labels = nci.labs, main = "Average Linkage", xlab = "", sub = "", ylab = "")
plot(hclust(data.dist, method = "single"), labels = nci.labs, main = "Single Linkage", xlab = "", sub = "", ylab = "")
```

We cut the tree to give us four clusters using [`cutree()`](http://bit.ly/R_cutree).

```{r}
hc.out <- hclust(dist(sd.data))
hc.clusters <- cutree(hc.out, 4)
table(hc.clusters, nci.labs)
```

And plot the results with four clusters.

```{r}
par(mfrow = c(1, 1))
plot(hc.out, labels = nci.labs)
abline(h = 139, col = "red")
```

We can get a summary of the result from the return value of [`hclust()`](http://bit.ly/R_hclust).

```{r}
hc.out
```

For clustering the cancer types in four groups with K-means, we simply run [`kmeans()`](http://bit.ly/R_kmeans) with `K = 4`.

```{r}
set.seed(2)
km.out <- kmeans(sd.data, 4, nstart = 20)
km.clusters <- km.out$cluster
table(km.clusters, hc.clusters)
```

We can also combine the different algorithms by first running principal component analysis and then performing hierarchical clustering on the first few principal components.

clustering after PCA is much less 'noisy'

```{r}
# PCA
pr.out <- prcomp(nci.data, scale = TRUE)
pve <- pr.out$sdev^2/sum(pr.out$sdev^2)
plot(pve, xlab = "Principal Component", ylab = "Proportion of Variance Explained ", type = "b", pch=16)
# Hierarchical clustering
hc.out <- hclust(dist(pr.out$x[, 1:8]))
plot(hc.out, labels = nci.labs, main = "Hier. Clust. on First Eight PCs", xlab='')
table(cutree(hc.out, 4), nci.labs)
# Now let's try k-means
km.out <- kmeans(dist(pr.out$x[, 1:8]), 6, nstart = 20)
table(km.out$cluster, nci.labs)
```

We can compute various metrics to assess each clustering method's performance.

```{r}
library(aricode)
ARI(km.out$cluster, nci.labs)
ARI(cutree(hc.out, 4), nci.labs)
ARI(cutree(hc.out, 10), nci.labs)

library(cluster)
silhouette_score <- function(k){
  km <- kmeans(nci.data, centers = k, nstart=25)
  ss <- silhouette(km$cluster, dist(nci.data))
  mean(ss[, 3])
}
k <- 2:10
avg_sil <- sapply(k, silhouette_score)
plot(k, type='b', avg_sil, xlab='Number of clusters', ylab='Average Silhouette Scores', frame=FALSE)

```


Spectral clustering
```{r}
library(kernlab)
data(spirals)
sc <- specc(spirals, centers=2)
plot(spirals, col=sc)
```
